{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 â€“ Model Selection & Cross Validation\n",
    "\n",
    "Yesterday, we engineered powerful features and saved them to encoded_data.csv. Today, we'll use that dataset to evaluate three different regression models and find the most reliable one using robust validation techniques.\n",
    "\n",
    "Objective:\n",
    "\n",
    "- Load the pre-engineered data from Day 3.\n",
    "\n",
    "- Evaluate Linear Regression, k-Nearest Neighbors (kNN), and Polynomial Regression.\n",
    "\n",
    "- Compare three validation strategies: Train-Test Split, Train-Validation-Test Split, and K-Fold Cross-Validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of loaded features (X): (425, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the engineered dataset from Day 3\n",
    "df = pd.read_csv('/Users/amrithaa/Downloads/12DayDataSeries/notebooks/data/encoded_data.csv')\n",
    "\n",
    "\n",
    "# Define the features (X) and target (y)\n",
    "# We select the most impactful engineered features\n",
    "features = [\n",
    "    'crew_size',\n",
    "    'mental_health_score',\n",
    "    'risk_score',\n",
    "    'mission_type_Rescue',\n",
    "    'mission_type_Resupply',\n",
    "    'duration_bin_medium',\n",
    "    'duration_bin_long'\n",
    "]\n",
    "target = 'mission_success_score'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Note: The numerical features were already scaled with StandardScaler in Day 3.\n",
    "# We'll proceed directly to modeling.\n",
    "\n",
    "print(\"Shape of loaded features (X):\", X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Train-Test Split (80/20)\n",
    "This is our first and simplest check. We split the data once to get a baseline for how our models perform on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "linear_reg = LinearRegression()\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=7) # Using k=7 as a reasonable starting point\n",
    "poly_reg = make_pipeline(PolynomialFeatures(degree=2, include_bias=False), LinearRegression())\n",
    "\n",
    "# Train models\n",
    "linear_reg.fit(X_train, y_train)\n",
    "knn_reg.fit(X_train, y_train)\n",
    "poly_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "mse_linear = mean_squared_error(y_test, linear_reg.predict(X_test))\n",
    "mse_knn = mean_squared_error(y_test, knn_reg.predict(X_test))\n",
    "mse_poly = mean_squared_error(y_test, poly_reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Train-Test Split\n",
      "                        Model  Test MSE\n",
      "            Linear Regression 17.245092\n",
      "         kNN Regression (k=7) 25.510034\n",
      "Polynomial Regression (deg=2) 16.005268\n"
     ]
    }
   ],
   "source": [
    "# Code to display results\n",
    "\n",
    "# Collect model names and their corresponding test MSE\n",
    "results = {\n",
    "    \"Model\": [\"Linear Regression\", \"kNN Regression (k=7)\", \"Polynomial Regression (deg=2)\"],\n",
    "    \"Test MSE\": [mse_linear, mse_knn, mse_poly]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for clean display\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results: Train-Test Split\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: On this single split, the Polynomial Regression model is the clear winner. This suggests there are valuable non-linear interactions between our engineered features that the simpler models can't capture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Train-Validation-Test Split (60/20/20)\n",
    "\n",
    "This is a better approach. The validation set allows us to tune model parameters (like finding the best k for kNN) without \"peeking\" at the final test set, which would bias our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 60% train, 20% validation, 20% test\n",
    "X_train_val, X_test_final, y_train_val, y_test_final = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train models on the 60% training set\n",
    "linear_reg.fit(X_train_final, y_train_final)\n",
    "knn_reg.fit(X_train_final, y_train_final)\n",
    "poly_reg.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Evaluate on the validation set (used for tuning)\n",
    "val_mse_linear = mean_squared_error(y_val, linear_reg.predict(X_val))\n",
    "val_mse_knn = mean_squared_error(y_val, knn_reg.predict(X_val))\n",
    "val_mse_poly = mean_squared_error(y_val, poly_reg.predict(X_val))\n",
    "\n",
    "# Final evaluation on the unseen test set\n",
    "final_mse_linear = mean_squared_error(y_test_final, linear_reg.predict(X_test_final))\n",
    "final_mse_knn = mean_squared_error(y_test_final, knn_reg.predict(X_test_final))\n",
    "final_mse_poly = mean_squared_error(y_test_final, poly_reg.predict(X_test_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Train-Validation-Test Split\n",
      "                        Model  Validation MSE  Final Test MSE\n",
      "            Linear Regression       24.155318       17.222822\n",
      "         kNN Regression (k=7)       29.383277       28.112484\n",
      "Polynomial Regression (deg=2)       23.795181       16.980122\n"
     ]
    }
   ],
   "source": [
    "# Code to display TV results\n",
    "\n",
    "# Collect validation and test MSE for each model\n",
    "results_tv = {\n",
    "    \"Model\": [\"Linear Regression\", \"kNN Regression (k=7)\", \"Polynomial Regression (deg=2)\"],\n",
    "    \"Validation MSE\": [val_mse_linear, val_mse_knn, val_mse_poly],\n",
    "    \"Final Test MSE\": [final_mse_linear, final_mse_knn, final_mse_poly]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_tv_df = pd.DataFrame(results_tv)\n",
    "\n",
    "# Display results\n",
    "print(\"Results: Train-Validation-Test Split\")\n",
    "print(results_tv_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: The results hold. The Polynomial Regressor performed best on both the validation and the final test sets. This consistency gives us greater confidence that it's the right model for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: K-Fold Cross-Validation (The Gold Standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "# Note: We use the full dataset (X, y) as cross_val_score handles the splits\n",
    "cv_scores_linear = -cross_val_score(linear_reg, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_scores_knn = -cross_val_score(knn_reg, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_scores_poly = -cross_val_score(poly_reg, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Calculate the average MSE\n",
    "avg_mse_linear = np.mean(cv_scores_linear)\n",
    "avg_mse_knn = np.mean(cv_scores_knn)\n",
    "avg_mse_poly = np.mean(cv_scores_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 5-Fold Cross-Validation\n",
      "                        Model                                                                                        CV Fold MSEs  Average MSE\n",
      "            Linear Regression [20.254832397999557, 21.231303364720834, 19.554667576047173, 21.98905701752795, 18.552247740787656]        20.32\n",
      "         kNN Regression (k=7) [29.512259591836738, 30.947534621848746, 25.82201126050422, 29.568015606242486, 26.498327539015587]        28.47\n",
      "Polynomial Regression (deg=2)  [18.708230746416785, 19.55556801858651, 20.51452139147108, 20.622538248239277, 15.366709977129435]        18.95\n"
     ]
    }
   ],
   "source": [
    "# Print results for CV \n",
    "\n",
    "# Create results dictionary\n",
    "cv_results = {\n",
    "    \"Model\": [\"Linear Regression\", \"kNN Regression (k=7)\", \"Polynomial Regression (deg=2)\"],\n",
    "    \"CV Fold MSEs\": [cv_scores_linear, cv_scores_knn, cv_scores_poly],\n",
    "    \"Average MSE\": [avg_mse_linear, avg_mse_knn, avg_mse_poly]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "\n",
    "# Format float precision for better readability\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "# Display results\n",
    "print(\"Results: 5-Fold Cross-Validation\")\n",
    "print(cv_results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: After averaging the performance across 5 different folds, the Polynomial Regression model is confirmed as the most reliable choice. Its average MSE of 18.95 is our most trustworthy estimate of how it will perform on future, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway\n",
    "\n",
    "| Evaluation Method | Linear Regression MSE | kNN Regression MSE | Polynomial Regression MSE | **Best Model** |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Train-Test Split (80/20)** | 17.25 | 25.51 | **16.01** | **Polynomial** |\n",
    "| **Train-Val-Test (60/20/20)**| 17.22 (Test) | 28.11 (Test) | **16.98** (Test) | **Polynomial** |\n",
    "| **5-Fold Cross-Validation**| 20.32 (Avg) | 28.47 (Avg) | **18.95** (Avg) | **Polynomial** |\n",
    "\n",
    "The story is clear and consistent across all evaluation methods:\n",
    "\n",
    "* **Feature Engineering Paid Off**: The powerful features we created in Day 3 gave our models a strong foundation. This is evident in the strong performance of even the simple Linear Regression model.\n",
    "* **The Right Complexity Matters**: The Polynomial Regression model consistently succeeded because it could capture the complex, non-linear interactions between our engineered features, giving it the winning edge. The kNN model was not well-suited for this particular problem.\n",
    "* **Cross-Validation is Trustworthy**: While the Polynomial model scored exceptionally well on single test sets (MSE of ~16-17), the cross-validation score gives us a more stable and reliable estimate of its true predictive power. We should expect its real-world performance to have an MSE closer to **18.95**.\n",
    "\n",
    "**Final Takeaway**: Yesterday we created the right ingredients. Today, we rigorously tested the recipes and found the one that works best. **Cross-validation is the ultimate taste test that proves a model is truly effective and not just lucky.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
